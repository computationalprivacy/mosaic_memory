{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An additional extraction expriment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/matthieu/miniconda/envs/mosaic/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "import torch\n",
    "from typing import Sequence\n",
    "from collections import defaultdict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start by loading a model with nrep=10 as upper bound\n",
    "\n",
    "SEED = 1\n",
    "NREP = 10\n",
    "\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "target_tokenizer.pad_token = target_tokenizer.eos_token\n",
    "\n",
    "# load the canaries\n",
    "with open(f\"./data/members_journal_gpt_seed{SEED}.pickle\", \"rb\") as f:\n",
    "    canaries = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.84s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = f\"SOME_DATA_DIR/EleutherAI_gpt-neo-1.3B_gptneo1B_exact_duplicates_gpt_seed{SEED}_nrep{NREP}_lr2e5\"\n",
    "target_device = \"cuda:0\"\n",
    "\n",
    "target_model = AutoModelForCausalLM.from_pretrained(model_path).to(target_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def compute_bleu(reference_str, hypothesis_str) -> float:\n",
    "\n",
    "    reference_tokens = word_tokenize(reference_str)\n",
    "    candidate_tokens = word_tokenize(hypothesis_str) # make sure to match lengths, as generation might be shorter/longer\n",
    "    \n",
    "    bleu_score = sentence_bleu([reference_tokens], candidate_tokens)\n",
    "    \n",
    "    return bleu_score\n",
    "\n",
    "def edit_distance(seq1: Sequence, seq2: Sequence) -> int:\n",
    "    '''on the token level'''\n",
    "    if len(seq1) > len(seq2):\n",
    "        seq1, seq2 = seq2, seq1\n",
    "        \n",
    "    prev_row = list(range(len(seq1) + 1))\n",
    "    curr_row = [0] * (len(seq1) + 1)\n",
    "    \n",
    "    for j in range(1, len(seq2) + 1):\n",
    "        curr_row[0] = j\n",
    "        \n",
    "        for i in range(1, len(seq1) + 1):\n",
    "            if seq1[i-1] == seq2[j-1]:\n",
    "                curr_row[i] = prev_row[i-1]\n",
    "            else:\n",
    "                curr_row[i] = min(prev_row[i-1] + 1,  # substitution\n",
    "                                prev_row[i] + 1,     # deletion\n",
    "                                curr_row[i-1] + 1)   # insertion\n",
    "        \n",
    "        prev_row, curr_row = curr_row, prev_row\n",
    "        \n",
    "    return prev_row[-1]\n",
    "\n",
    "def edit_similarity(seq1: Sequence, seq2: Sequence) -> float:\n",
    "    \n",
    "    edit_dist = edit_distance(seq1, seq2)\n",
    "    max_len = max(len(seq1), len(seq2))\n",
    "    return 1 - (edit_dist / max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_extractability(model, tokenizer,\n",
    "    input_ids: torch.Tensor,\n",
    "    attention_mask: torch.Tensor = None,\n",
    "    secret_len: int = 1,\n",
    "    verbose=False\n",
    "):\n",
    "    total_length = input_ids.shape[1]\n",
    "    prompt_lenght = total_length - secret_len\n",
    "\n",
    "    prompt_tokens = input_ids[:, :prompt_lenght]\n",
    "    attention_mask = attention_mask[:, :prompt_lenght]\n",
    "    secret_tokens = input_ids[:, prompt_lenght:]\n",
    "\n",
    "    greedy_output = model.generate(\n",
    "        inputs=prompt_tokens,\n",
    "        max_length=total_length,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        attention_mask=attention_mask,\n",
    "    )\n",
    "    \n",
    "    # let's decode it\n",
    "    original_prompt = tokenizer.batch_decode(prompt_tokens, skip_special_tokens=True)\n",
    "    original_secret_text = tokenizer.batch_decode(secret_tokens, skip_special_tokens=True)\n",
    "    generated_text = tokenizer.batch_decode(greedy_output[:, prompt_lenght:], skip_special_tokens=True)\n",
    "\n",
    "    all_bleu_scores = []\n",
    "    all_edit_sims = []\n",
    "    for i in range(len(greedy_output)):\n",
    "        \n",
    "        bleu = compute_bleu(original_secret_text[i], generated_text[i])\n",
    "        edit_sim = edit_similarity(\n",
    "            secret_tokens[i].cpu().numpy().tolist(),\n",
    "            greedy_output[i, prompt_lenght:].cpu().numpy().tolist()\n",
    "        )\n",
    "        \n",
    "        all_bleu_scores.append(bleu)\n",
    "        all_edit_sims.append(edit_sim)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Original prompt')\n",
    "            print(original_prompt[i])\n",
    "            print('---')\n",
    "            print('Original text')\n",
    "            print(original_secret_text[i])\n",
    "            print('---')\n",
    "            print('Generated text')\n",
    "            print(generated_text[i])\n",
    "                \n",
    "            print('BLEU score:', bleu)        \n",
    "            print('Edit distance:', edit_sim)\n",
    "            print('=======')\n",
    "\n",
    "    return all_bleu_scores, all_edit_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do this for all canaries\n",
    "\n",
    "# canaries is a list of 100 canaries of 100 tokens each, so already encoded\n",
    "input_ids = torch.tensor(canaries).to(target_device)\n",
    "attention_mask = (input_ids != target_tokenizer.pad_token_id).long().to(target_device)\n",
    "\n",
    "all_bleu_scores, all_edit_sims = compute_extractability(\n",
    "    model=target_model,\n",
    "    tokenizer=target_tokenizer,\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    secret_len=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean BLEU score: 0.1750160768175124 std: 0.24549921118754028\n",
      "Max BLEU score: 1.0\n",
      "Mean Edit sim: 0.20939999999999998 std: 0.2298426418226174\n",
      "Max Edit sim: 1.0\n"
     ]
    }
   ],
   "source": [
    "# print mean and max\n",
    "print('Mean BLEU score:', np.mean(all_bleu_scores), 'std:', np.std(all_bleu_scores))\n",
    "print('Max BLEU score:', np.max(all_bleu_scores))\n",
    "print('Mean Edit sim:', np.mean(all_edit_sims), 'std:', np.std(all_edit_sims))\n",
    "print('Max Edit sim:', np.max(all_edit_sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 13.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrep=10\n",
      "Mean BLEU score: 0.1750160768175124 std: 0.24549921118754028\n",
      "Mean Edit sim: 0.20939999999999998 std: 0.2298426418226174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/matthieu/miniconda/envs/mosaic/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/data/matthieu/miniconda/envs/mosaic/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/data/matthieu/miniconda/envs/mosaic/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# first do this for the exact models\n",
    "nreps = (1, 10,)\n",
    "seeds = (1, )\n",
    "\n",
    "for nrep in nreps:\n",
    "    \n",
    "    all_bleu_scores = []\n",
    "    all_edit_sims = []\n",
    "    \n",
    "    for seed in seeds:\n",
    "    \n",
    "        # load the model\n",
    "        model_path = f\"SOME_DATA_DIR/EleutherAI_gpt-neo-1.3B_gptneo1B_exact_duplicates_gpt_seed{seed}_nrep{nrep}_lr2e5\"\n",
    "        target_model = AutoModelForCausalLM.from_pretrained(model_path).to(target_device)\n",
    "        \n",
    "        # load the canaries\n",
    "        with open(f\"SOME_DATA_DIR/members_seed{seed}.pickle\", \"rb\") as f:\n",
    "            canaries = pickle.load(f)\n",
    "            \n",
    "        input_ids = torch.tensor(canaries).to(target_device)\n",
    "        attention_mask = (input_ids != target_tokenizer.pad_token_id).long().to(target_device)\n",
    "        \n",
    "        seed_bleu_scores, seed_edit_sims = compute_extractability(\n",
    "            model=target_model,\n",
    "            tokenizer=target_tokenizer,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            secret_len=50,\n",
    "        )\n",
    "        \n",
    "        all_bleu_scores.extend(seed_bleu_scores)\n",
    "        all_edit_sims.extend(seed_edit_sims)\n",
    "        \n",
    "    print(f'nrep={nrep}')\n",
    "    print('Mean BLEU score:', np.mean(all_bleu_scores), 'std:', np.std(all_bleu_scores))\n",
    "    print('Mean Edit sim:', np.mean(all_edit_sims), 'std:', np.std(all_edit_sims))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  33%|███▎      | 1/3 [00:29<00:58, 29.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:13<00:00, 24.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R=1\n",
      "Mean BLEU score: 0.27648182593829146 std: 0.2618339985391867\n",
      "Mean Edit sim: 0.2988 std: 0.26232529424361656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:46<00:00, 15.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R=5\n",
      "Mean BLEU score: 0.23656901925427076 std: 0.23652144118811755\n",
      "Mean Edit sim: 0.2706 std: 0.23767970043737435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:44<00:00, 14.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R=10\n",
      "Mean BLEU score: 0.16965976280805767 std: 0.2010352439100575\n",
      "Mean Edit sim: 0.20260000000000006 std: 0.1946413111340961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:39<00:00, 13.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R=15\n",
      "Mean BLEU score: 0.1416696296490674 std: 0.20021217234425376\n",
      "Mean Edit sim: 0.17840000000000003 std: 0.18669076034983628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:51<00:00, 17.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R=20\n",
      "Mean BLEU score: 0.11093556054925362 std: 0.1738619390833732\n",
      "Mean Edit sim: 0.15660000000000002 std: 0.1810426469095058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:50<00:00, 16.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R=25\n",
      "Mean BLEU score: 0.10588469671678759 std: 0.19301640133018072\n",
      "Mean Edit sim: 0.15860000000000002 std: 0.18519730019630418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:41<00:00, 13.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R=50\n",
      "Mean BLEU score: 0.07309259352906256 std: 0.1573960144231942\n",
      "Mean Edit sim: 0.12380000000000001 std: 0.15501470897950295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:40<00:00, 13.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R=75\n",
      "Mean BLEU score: 0.06925742476193107 std: 0.14212570307405026\n",
      "Mean Edit sim: 0.12459999999999999 std: 0.14944845265174211\n"
     ]
    }
   ],
   "source": [
    "# now for the fuzzy dupls\n",
    "\n",
    "Rs = [1, 5, 10, 15, 20, 25, 50, 75]\n",
    "seeds = (1, )\n",
    "\n",
    "for R in Rs:\n",
    "    \n",
    "    all_bleu_scores = []\n",
    "    all_edit_sims = []\n",
    "    \n",
    "    for seed in seeds:\n",
    "    \n",
    "        # load the model \n",
    "        model_path = f\"SOME_DATA_DIR/EleutherAI_gpt-neo-2.7B_gptneo2.7B_near_duplicates_diff_indices_topk10_gpt_R{R}_seed{seed}_lr2e5\"\n",
    "        target_model = AutoModelForCausalLM.from_pretrained(model_path).to(target_device)\n",
    "        \n",
    "        # load the canaries\n",
    "        with open(f\"./data/members_journal_gpt_seed{seed}.pickle\", \"rb\") as f:\n",
    "            canaries = pickle.load(f)\n",
    "            \n",
    "        input_ids = torch.tensor(canaries).to(target_device)\n",
    "        attention_mask = (input_ids != target_tokenizer.pad_token_id).long().to(target_device)\n",
    "        \n",
    "        seed_bleu_scores, seed_edit_sims = compute_extractability(\n",
    "            model=target_model,\n",
    "            tokenizer=target_tokenizer,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            secret_len=50,\n",
    "        )\n",
    "        \n",
    "        all_bleu_scores.extend(seed_bleu_scores)\n",
    "        all_edit_sims.extend(seed_edit_sims)\n",
    "        \n",
    "    print(f'R={R}')\n",
    "    print('Mean BLEU score:', np.mean(all_bleu_scores), 'std:', np.std(all_bleu_scores))\n",
    "    print('Mean Edit sim:', np.mean(all_edit_sims), 'std:', np.std(all_edit_sims))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mosaic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
