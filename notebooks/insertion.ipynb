{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df7fdf89-800a-466c-b221-727195392c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthieu/miniconda3/envs/canary/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import pickle \n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e32d63f-685f-4d26-b179-f23456fe916a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_idx</th>\n",
       "      <th>book_title</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>n_repetitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A Letter to John Wilkes, Esq.</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>London in the Time of the Tudors</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The American Missionary -- Volume 37, No. 7, J...</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Brass Check</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Birds of Song and Story</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>The Ivory Tower</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>Retrospective exhibition of important works of...</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>John Cheap, the Chapman's Library. Vol. 2: Rel...</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>The works of the Rev. John Wesley, Vol. 05 (of...</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>In a Yellow Wood</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    book_idx                                         book_title  \\\n",
       "0          0                      A Letter to John Wilkes, Esq.   \n",
       "1          1                   London in the Time of the Tudors   \n",
       "2          2  The American Missionary -- Volume 37, No. 7, J...   \n",
       "3          3                                    The Brass Check   \n",
       "4          4                            Birds of Song and Story   \n",
       "..       ...                                                ...   \n",
       "95        95                                    The Ivory Tower   \n",
       "96        96  Retrospective exhibition of important works of...   \n",
       "97        97  John Cheap, the Chapman's Library. Vol. 2: Rel...   \n",
       "98        98  The works of the Rev. John Wesley, Vol. 05 (of...   \n",
       "99        99                                   In a Yellow Wood   \n",
       "\n",
       "    sequence_length  n_repetitions  \n",
       "0               100             10  \n",
       "1               100             10  \n",
       "2               100             10  \n",
       "3               100             10  \n",
       "4               100             10  \n",
       "..              ...            ...  \n",
       "95              100             10  \n",
       "96              100             10  \n",
       "97              100             10  \n",
       "98              100             10  \n",
       "99              100             10  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 100\n",
    "repetitions = 10\n",
    "\n",
    "book_dataset = load_from_disk(\"SOME_DATA_DIR/clean_books_to_inject_neardupl_100\")\n",
    "all_titles = []\n",
    "\n",
    "for i in range(len(book_dataset)):\n",
    "    all_titles.append([int(i), book_dataset[i]['book_title'], seq_length, repetitions])\n",
    "\n",
    "df = pd.DataFrame(all_titles, columns = ['book_idx', 'book_title', 'sequence_length', 'n_repetitions'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0825f045-b23e-49da-b900-6d61af66f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc4cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get the og canaries\n",
    "OG_CANARY_PATH = \"SOME_DATA_DIR/members.pickle\"\n",
    "\n",
    "with open(OG_CANARY_PATH, 'rb') as f:\n",
    "    og_canaries = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c13e9ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_near_dupl_canary(og_text: str, all_canary_tokens: list, tokenizer: AutoTokenizer) -> str:\n",
    "    '''\n",
    "    Let's inject the canary at random places in the original text. \n",
    "    By splitting on spaces, we ensure to inject the canaries while not splitting any words from the original text.\n",
    "    '''\n",
    "\n",
    "    book_split_by_spaces = og_text.split(\" \")\n",
    "    all_indices_book = range(len(book_split_by_spaces))\n",
    "    canary_indices = random.sample(all_indices_book, len(all_canary_tokens))\n",
    "    canary_indices_sorted = np.sort(canary_indices)\n",
    "\n",
    "    new_text = ''\n",
    "    last_index = 0\n",
    "\n",
    "    all_canary_length = 0\n",
    "\n",
    "    for i, idx in enumerate(canary_indices_sorted):\n",
    "        canary_tokens = all_canary_tokens[i]\n",
    "        canary = tokenizer.decode(canary_tokens)\n",
    "        all_canary_length += len(canary)\n",
    "        new_text += \" \".join(book_split_by_spaces[last_index:idx])\n",
    "        if idx == 0:\n",
    "            new_text += canary \n",
    "        else:\n",
    "            new_text += \" \" + canary \n",
    "        last_index = idx\n",
    "\n",
    "    new_text += \" \".join(book_split_by_spaces[last_index:])\n",
    "\n",
    "    assert len(new_text)  == len(og_text) + all_canary_length\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d38003",
   "metadata": {},
   "source": [
    "# Let's start by inserting random tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9125894d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I read The Thin Red Line when I was in 11th grade. The movie was a little disappointing, but the book was amazing. I read it in 2006, so I'd forgotten most of the details. I went through and re-read it and it was still amazing, even in places where it seemed clunky and disjointed. The characters felt so real and the writing so visceral that I could still picture what was happening in the same way I do my favorite scenes from Catcher in\n",
      "I read The divest anx Thin Red LineYou coasts when I was Oz Vi in 11th Happy appreciate grade. The tcp guilty movie was a Carlton objectionable little disappointing,Questionsulner but the book Contemporaryertility was amazing. waones I read it begins Brandon in 2006, WWE Agu so I'dBOOKued forgotten most of MachinaMicro the details. delineAlbert I went through curtail ...\" and re-)* Maderead it and KurdistanAvoid it was still attachmentive amazing, evenstretched homeless in places where boutique294 it seemed cl prophet Siliconunky and disportation Tarjointed. radiator Hunting The characters felt faces losing so real andRecomm automatically the writing so incumbent Kitchen visceral that Ibos landlord could still pictureimum kb what was happening Coinszai in the same Intelinventory way I do Lin Stoke my favorite scenesONS Blend from Catcher Grants behold in\n"
     ]
    }
   ],
   "source": [
    "def get_n_grams_w_insertions(seq, n, n_insertions, vocab_size):\n",
    "    result = []\n",
    "    \n",
    "    for i in range(0, len(seq), n):\n",
    "        if i == 0:\n",
    "            # just add the first chunk\n",
    "            result += seq[i:i+n] \n",
    "        else:\n",
    "            # add X random tokens\n",
    "            result += random.sample(range(vocab_size), n_insertions)\n",
    "            # add the real n-grams\n",
    "            result += seq[i:i+n]\n",
    "    return result\n",
    "\n",
    "# test this\n",
    "print(tokenizer.decode(og_canaries[1]))\n",
    "\n",
    "result_w_insertions = get_n_grams_w_insertions(og_canaries[1], n=3, \n",
    "                                               n_insertions=2, vocab_size=tokenizer.vocab_size)\n",
    "\n",
    "print(tokenizer.decode(result_w_insertions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5232c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.25it/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 416.27 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:05<00:00, 19.39it/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 481.39 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 34.04it/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 311.04 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 45.63it/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 505.71 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 54.11it/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 477.67 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 62.41it/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 458.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "all_ns = [1, 2, 5, 10, 20, 50]\n",
    "X_insertions = 10\n",
    "\n",
    "for n in all_ns:\n",
    "    \n",
    "    canary_dataset_entries = []\n",
    "    \n",
    "    for i in tqdm(range(len(book_dataset))):\n",
    "        og_entry = book_dataset[i]\n",
    "                \n",
    "        all_canary_chunks = [] \n",
    "\n",
    "        # first let's add the original one as we do this for all others too\n",
    "        original = og_canaries[i]\n",
    "        all_canary_chunks.append(original)\n",
    "\n",
    "        # now create all canaries with insertions\n",
    "        for _ in range(9):\n",
    "            n_gram_w_insertions = get_n_grams_w_insertions(original, n=n, \n",
    "                                                n_insertions=X_insertions, vocab_size=tokenizer.vocab_size)\n",
    "            all_canary_chunks.append(n_gram_w_insertions)\n",
    "                \n",
    "        new_text = inject_near_dupl_canary(og_text=og_entry[\"text\"], all_canary_tokens=all_canary_chunks, tokenizer=tokenizer)\n",
    "        \n",
    "        new_entry = og_entry.copy()\n",
    "        new_entry[\"text\"] = new_text\n",
    "            \n",
    "        canary_dataset_entries.append(new_entry)\n",
    "\n",
    "    # save the results\n",
    "    dataset = Dataset.from_dict({\"title\": [entry[\"book_title\"] for entry in canary_dataset_entries],\n",
    "                                \"text\": [entry[\"text\"] for entry in canary_dataset_entries]})\n",
    "        \n",
    "    dataset.save_to_disk(f'SOME_DATA_DIR/books_w_neardupl_canaries_decoder_ngrams_insertions_n{n}_X_insert{X_insertions}_100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d16374",
   "metadata": {},
   "source": [
    "## Now let's do the lower baseline, ie spreading it randomly across the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2797d470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 17256, 366, 19282, 1878],\n",
       " [87, 13, 71, 1, 198],\n",
       " [2, 17256, 366, 14881, 14],\n",
       " [14881, 37372, 13, 71, 1],\n",
       " [198, 198, 14933, 10223, 7308],\n",
       " [198, 90, 628, 220, 220],\n",
       " [220, 493, 2624, 62, 83],\n",
       " [493, 2514, 13290, 7, 600],\n",
       " [2624, 62, 83, 287, 8],\n",
       " [198, 220, 220, 220, 1391],\n",
       " [198, 220, 220, 220, 220],\n",
       " [220, 220, 220, 1441, 838],\n",
       " [1635, 287, 1343, 4764, 26],\n",
       " [198, 220, 220, 220, 1782],\n",
       " [628, 220, 220, 220, 493],\n",
       " [2624, 62, 83, 493, 2514],\n",
       " [13290, 7, 22468, 287, 8],\n",
       " [198, 220, 220, 220, 1391],\n",
       " [198, 220, 220, 220, 220],\n",
       " [220, 220, 220, 1441, 838]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_n_grams(seq, n):\n",
    "    n_grams = [seq[i:i+n] for i in range(0, len(seq), n)]\n",
    "    return n_grams\n",
    "\n",
    "split_n_grams(og_canaries[8], n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce2bf4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ns = [1, 2, 5, 10, 20, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44348736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 48.81it/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 578.48 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 59.60it/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 632.07 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 67.42it/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 651.16 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 70.10it/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 620.42 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 73.11it/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 663.56 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 75.17it/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 658.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "for n in all_ns:\n",
    "    \n",
    "    canary_dataset_entries = []\n",
    "    \n",
    "    for i in tqdm(range(len(book_dataset))):\n",
    "        og_entry = book_dataset[i]\n",
    "            \n",
    "        all_canary_chunks = [] \n",
    "\n",
    "        # first let's add the original one as we do this for all others too\n",
    "        original = og_canaries[i]\n",
    "        all_canary_chunks.append(original)\n",
    "\n",
    "        # now create all n-grams\n",
    "        n_grams = split_n_grams(original, n)\n",
    "        for _ in range(9):\n",
    "            all_canary_chunks += n_grams\n",
    "            \n",
    "        # now also shuffle them\n",
    "        random.shuffle(all_canary_chunks)\n",
    "                \n",
    "        new_text = inject_near_dupl_canary(og_text=og_entry[\"text\"], all_canary_tokens=all_canary_chunks, tokenizer=tokenizer)\n",
    "        \n",
    "        new_entry = og_entry.copy()\n",
    "        new_entry[\"text\"] = new_text\n",
    "            \n",
    "        canary_dataset_entries.append(new_entry)\n",
    "\n",
    "    # save the results\n",
    "    dataset = Dataset.from_dict({\"title\": [entry[\"book_title\"] for entry in canary_dataset_entries],\n",
    "                                \"text\": [entry[\"text\"] for entry in canary_dataset_entries]})\n",
    "        \n",
    "    dataset.save_to_disk(f'SOME_DATA_DIR/books_w_neardupl_canaries_decoder_ngrams_scrambled_n{n}_100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25219d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
