{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import permutations\n",
    "from collections import Counter, defaultdict\n",
    "import torch\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from random import shuffle, sample\n",
    "\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import pickle \n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kendall_tau_distance(rank1, rank2, normalized=False):\n",
    "    \"\"\"\n",
    "    Calculate the Kendall tau distance between two rankings.\n",
    "    \n",
    "    Args:\n",
    "        rank1, rank2: Lists or arrays of the same length containing rankings\n",
    "        normalized: If True, normalizes the distance to [0,1] range by dividing\n",
    "                   by the maximum possible distance n*(n-1)/2\n",
    "        \n",
    "    Returns:\n",
    "        float: Kendall tau distance (normalized if normalized=True)\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If inputs have different lengths or contain invalid rankings\n",
    "    \"\"\"\n",
    "    if len(rank1) != len(rank2):\n",
    "        raise ValueError(\"Rankings must have equal length\")\n",
    "        \n",
    "    n = len(rank1)\n",
    "    discordant_pairs = 0\n",
    "    \n",
    "    for i in range(n-1):\n",
    "        for j in range(i+1, n):\n",
    "            if (rank1[i] < rank1[j] and rank2[i] > rank2[j]) or \\\n",
    "               (rank1[i] > rank1[j] and rank2[i] < rank2[j]):\n",
    "                discordant_pairs += 1\n",
    "    \n",
    "    if normalized:\n",
    "        max_distance = n * (n - 1) / 2\n",
    "        return discordant_pairs / max_distance\n",
    "                \n",
    "    return discordant_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(ngram_rank, n_tokens=100):\n",
    "    n = len(ngram_rank)\n",
    "    \n",
    "    if n_tokens % n != 0:\n",
    "        raise ValueError(\"n_tokens must be divisible by the length of ngram_rank\")\n",
    "    \n",
    "    if sorted(ngram_rank) != list(range(n)):\n",
    "        raise ValueError(\"ngram_rank must contain consecutive ranks starting from 0\")\n",
    "    \n",
    "    ngram_size = n_tokens // n\n",
    "    \n",
    "    ret = []\n",
    "    for i in ngram_rank:\n",
    "        for j in range(ngram_size):\n",
    "            ret.append(i*ngram_size + j)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kendall_perturbations2(ngram_size, n_tokens, n_perturbations=9):\n",
    "    og_token_rank = list(range(n_tokens))\n",
    "    ngram_rank = list(range(n_tokens // ngram_size))\n",
    "\n",
    "    if ngram_size in (2,5):\n",
    "        target_dist = np.linspace(0.1,0.9,9)\n",
    "    elif ngram_size == 10:\n",
    "        target_dist = np.linspace(0.2,0.8,7)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    \n",
    "    perms = {round(k,1):[] for k in target_dist}\n",
    "\n",
    "    for target_dist in perms:\n",
    "        ret = []\n",
    "        while len(perms[target_dist]) < n_perturbations:\n",
    "            if target_dist > 0.5:\n",
    "                ngram_rank_shuffled = list(reversed(ngram_rank))\n",
    "            else:\n",
    "                ngram_rank_shuffled = ngram_rank.copy()\n",
    "            n = len(ngram_rank_shuffled)\n",
    "            for _ in range(n*n):\n",
    "                i,j = sample(range(n), 2)\n",
    "                ngram_rank_shuffled[i], ngram_rank_shuffled[j] = ngram_rank_shuffled[j], ngram_rank_shuffled[i]\n",
    "\n",
    "                perturbed_token_rank = perturb(ngram_rank_shuffled, n_tokens=n_tokens)\n",
    "                distance = kendall_tau_distance(og_token_rank, perturbed_token_rank, normalized=True)\n",
    "                distance = round(math.floor(distance*100) / 100, 2)\n",
    "                # print(distance)\n",
    "\n",
    "                if distance == target_dist:\n",
    "                    perms[target_dist].append(perturbed_token_rank)\n",
    "                    # print(target_dist, \"+1\")\n",
    "                    break\n",
    "                if target_dist <= 0.5 and distance > target_dist:\n",
    "                    break\n",
    "                if target_dist > 0.5 and distance < target_dist:\n",
    "                    break\n",
    "                \n",
    "            # print(\"---------\")\n",
    "        # print(target_dist, \"done\")\n",
    "              \n",
    "    return perms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_idx</th>\n",
       "      <th>book_title</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>n_repetitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A Letter to John Wilkes, Esq.</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>London in the Time of the Tudors</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The American Missionary -- Volume 37, No. 7, J...</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Brass Check</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Birds of Song and Story</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>The Ivory Tower</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>Retrospective exhibition of important works of...</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>John Cheap, the Chapman's Library. Vol. 2: Rel...</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>The works of the Rev. John Wesley, Vol. 05 (of...</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>In a Yellow Wood</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    book_idx                                         book_title  \\\n",
       "0          0                      A Letter to John Wilkes, Esq.   \n",
       "1          1                   London in the Time of the Tudors   \n",
       "2          2  The American Missionary -- Volume 37, No. 7, J...   \n",
       "3          3                                    The Brass Check   \n",
       "4          4                            Birds of Song and Story   \n",
       "..       ...                                                ...   \n",
       "95        95                                    The Ivory Tower   \n",
       "96        96  Retrospective exhibition of important works of...   \n",
       "97        97  John Cheap, the Chapman's Library. Vol. 2: Rel...   \n",
       "98        98  The works of the Rev. John Wesley, Vol. 05 (of...   \n",
       "99        99                                   In a Yellow Wood   \n",
       "\n",
       "    sequence_length  n_repetitions  \n",
       "0               100             10  \n",
       "1               100             10  \n",
       "2               100             10  \n",
       "3               100             10  \n",
       "4               100             10  \n",
       "..              ...            ...  \n",
       "95              100             10  \n",
       "96              100             10  \n",
       "97              100             10  \n",
       "98              100             10  \n",
       "99              100             10  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 100\n",
    "repetitions = 10\n",
    "\n",
    "book_dataset = load_from_disk(\"SOME_DATA_DIR/clean_books_to_inject_neardupl_100\")\n",
    "all_titles = []\n",
    "\n",
    "for i in range(len(book_dataset)):\n",
    "    all_titles.append([int(i), book_dataset[i]['book_title'], seq_length, repetitions])\n",
    "\n",
    "df = pd.DataFrame(all_titles, columns = ['book_idx', 'book_title', 'sequence_length', 'n_repetitions'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get the og canaries\n",
    "OG_CANARY_PATH = \"SOME_DATA_DIR/members.pickle\"\n",
    "\n",
    "with open(OG_CANARY_PATH, 'rb') as f:\n",
    "    og_canaries = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_near_dupl_canary(og_text: str, all_canary_tokens: list, tokenizer: AutoTokenizer) -> str:\n",
    "    '''\n",
    "    Let's inject the canary at random places in the original text. \n",
    "    By splitting on spaces, we ensure to inject the canaries while not splitting any words from the original text.\n",
    "    '''\n",
    "\n",
    "    book_split_by_spaces = og_text.split(\" \")\n",
    "    all_indices_book = range(len(book_split_by_spaces))\n",
    "    canary_indices = random.sample(all_indices_book, len(all_canary_tokens))\n",
    "    canary_indices_sorted = np.sort(canary_indices)\n",
    "\n",
    "    new_text = ''\n",
    "    last_index = 0\n",
    "\n",
    "    all_canary_length = 0\n",
    "\n",
    "    for i, idx in enumerate(canary_indices_sorted):\n",
    "        canary_tokens = all_canary_tokens[i]\n",
    "        canary = tokenizer.decode(canary_tokens)\n",
    "        all_canary_length += len(canary)\n",
    "        new_text += \" \".join(book_split_by_spaces[last_index:idx])\n",
    "        if idx == 0:\n",
    "            new_text += canary \n",
    "        else:\n",
    "            new_text += \" \" + canary \n",
    "        last_index = idx\n",
    "\n",
    "    new_text += \" \".join(book_split_by_spaces[last_index:])\n",
    "\n",
    "    assert len(new_text)  == len(og_text) + all_canary_length\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"SOME_DATA_DIR\"\n",
    "DS_NAME_TEMPLATE = \"kendall_dist_0{kendall_dist}_ngram_{ngram_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 100/100 [05:00<00:00,  3.01s/it]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 673.36 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 735.28 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 668.03 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 673.79 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 663.95 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 622.49 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 612.21 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 670.15 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 645.69 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 100/100 [04:11<00:00,  2.51s/it]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 636.51 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 675.44 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 760.79 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 709.97 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 713.82 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 716.06 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 689.26 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 758.40 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 687.91 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 100/100 [01:51<00:00,  1.11s/it]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 721.75 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 785.69 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 720.27 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 736.38 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 699.09 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 724.80 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 743.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "for ngram_size in (2,5,10):\n",
    "    canary_dataset_entries = defaultdict(list)\n",
    "    print(ngram_size)\n",
    "    for i in tqdm(range(len(book_dataset))):\n",
    "        og_entry = book_dataset[i]\n",
    "        perms = build_kendall_perturbations2(ngram_size, n_tokens=100, n_perturbations=9)\n",
    "\n",
    "        for kendall_dist in perms:\n",
    "            all_canary_chunks = []\n",
    "            original = og_canaries[i]  \n",
    "            all_canary_chunks.append(original)\n",
    "\n",
    "            for perm in perms[kendall_dist]:\n",
    "                perturbed = np.array(original)[perm]\n",
    "                all_canary_chunks.append(perturbed)\n",
    "            \n",
    "            new_text = inject_near_dupl_canary(og_text=og_entry[\"text\"], all_canary_tokens=all_canary_chunks, tokenizer=tokenizer)\n",
    "            new_entry = og_entry.copy()\n",
    "            new_entry[\"text\"] = new_text\n",
    "            \n",
    "            canary_dataset_entries[kendall_dist].append(new_entry)\n",
    "\n",
    "    for kendall_dist in canary_dataset_entries:\n",
    "        entries = canary_dataset_entries[kendall_dist]\n",
    "        dataset = Dataset.from_dict({\"title\": [entry[\"book_title\"] for entry in entries],\n",
    "                                    \"text\": [entry[\"text\"] for entry in entries]})\n",
    "        path = os.path.join(BASE_PATH, DS_NAME_TEMPLATE.format(kendall_dist=int(kendall_dist*10), ngram_size=ngram_size))\n",
    "            \n",
    "        dataset.save_to_disk(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
